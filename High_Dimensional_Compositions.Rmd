---
title: "When is a composition not a composition?"
slug: "INSERT_SLUG_HERE"
author: "Dominic LaRoche"
date: "YYYY-MM-DD"
output: markdowntemplates::hugo
---

```{r setup, include=FALSE}
library(knitr)


```



# When can I ignore the compositional property of my data?

My work involves the analysis of data generated by next-generation sequencing (NGS) platforms.  Data from NGS platforms generally falls under the umbrella of 'high dimensional compositions'.  I have often$*$ been asked how severe the constant sum constraint is for high high dimensional data with such a large constraint, when can we safely ignore the compositional nature of the data? ($*$ O.K. *often* is entirely relative here!).  This post is an attempt to answer that question.

Intuitively, if we increase the constraint to infinity we no longer have a constraint.  Is there some constraint $< \infty$ for which a composition ceses to be a composition for all practical purposes?

Also, if you slice up a composition into nearly infinite parts, how much depedence would one expect between the parts.  Intuitively, this dependence would decrease with increasing parts.

So let's put some math behind the intuition and try to answer this question.


## Compositions of very high dimension

Let's start with the dimension question.  How many components are necessary to safely ignore the interdependence between the components?  the problem arises from what John Aitchison described as the "negative bias problem" (note: this is not bias in the traditional estimation sense).  We start with the recognition that every D-part composition can be rescaled such that the components sum to 1.

$$x_1 + x_2 + \ldots + x_D = 1$$
From the definition of covariance and the sum constraint we have, 

$$cov(x_1,\text{ }x_1 + \ldots + x_D) = 0,$$
i.e. the covariance of any individual component with the sum of all the components is 0 (since the covariance of any random variable with a constant is 0).  Recall that the definition of covariance is $Cov(X, Y) = E((X - \mu_x)(Y - \mu_y))$.    

Therefore, 

$$cov(x_1, x_2) + cov(x_1,x_3) + \ldots + cov(x_1, x_D) = -var(x_1)$$




