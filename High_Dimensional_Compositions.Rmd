---
title: "When is a composition not a composition?"
slug: "INSERT_SLUG_HERE"
author: "Dominic LaRoche"
date: "YYYY-MM-DD"
output: markdowntemplates::hugo
---

```{r setup, include=FALSE}
library(knitr)


```



# When can I ignore the compositional property of my data?

My work involves the analysis of data generated by next-generation sequencing (NGS) platforms.  Data from NGS platforms generally falls under the umbrella of 'high dimensional compositions' (for a review of compositions see me previous post on the topic).  This type of data is characterized by very large sum constraints (10 to 250 million reads) and large numbers of components (in the thousands).  I have often$^1$ been asked how severe the constant sum constraint is for high dimensional data with such a large constraint and so many components, when can we safely ignore the compositional nature of the data?  This series of posts is an attempt to answer those questions.

Intuitively, if we increase the constraint to infinity we no longer have a constraint.  Is there some constraint $< \infty$ for which a composition ceases to be a composition for all practical purposes?

Also, if you slice up a composition into nearly infinite parts, how much depedence would one expect between the parts.  Intuitively, this dependence would decrease with increasing parts.

So let's put some math behind the intuition and try to answer this question.


## Compositions of very high dimension

Let's start with the dimension question.  How many components are necessary to safely ignore the interdependence between the components?  The problem arises from what John Aitchison described as the "negative bias problem" (note: this is not bias in the traditional estimation sense).  We start with the recognition that every D-part composition can be rescaled by a constant such that the components sum to 1 (*hint - this is forshadowing the discussion of the large sum constraint*).

$$x_1 + x_2 + \ldots + x_D = 1$$

From the definition of covariance ($\text{cov}(X, Y) = \text{E}((X - \mu_x)(Y - \mu_y))$) and the sum constraint we have, 

$$\text{cov}(x_1,\text{ }x_1 + \ldots + x_D) = 0,$$

i.e. the covariance of any individual component with the sum of all the components is 0 (since the covariance of any random variable with a constant is 0).  Moreover, 

$$\text{cov}(x_1,\text{ }x_2 + \ldots + x_D) = \text{cov}(x_1,\text{ }1 - x_1) = -\text{var}(x),$$

which also implies, 

$$\text{cov}(x_1,\text{ }x_2) + \text{cov}(x_1,\text{ }x_3) + \ldots + \text{cov}(x_1,\text{ }x_D) = -\text{var}(x)^{**2}$$

Essenially, every row of the covariance matrix *must* have negative values! This means that at least some covariances are constrained to be negative -- __which means we can't interpret the covariances in the traditional sense__.  As you can see from the above equalities, the 'negative bias' of the covariance matrix does not depend on the dimension, $D$.  


OK, we can't escape the problem of inter-dependence between components entirely, but maybe the problem becomes less severe as $D$ increases?   I will attempt to answer this with a simulation.  We first generate a small composition with a relatively large sum constraint.

```{r Simulate low dimension problem}
set.seed(1014)

# Generate random probability vector of 10 components
probs <- runif(10)
probs <- probs/sum(probs)

# Generate relative abundances of the 10 components
X <- rmultinom(n = 100, 
               size = 10000, 
               prob = probs)

# Calculate the variance-covariance matrix
covX <- cov(t(X))

# Variances
diag(covX)

# Show the negative bias
diag(covX) <- 0
rowSums(covX)

```

As predicted, the sum of every row of the covariance matrix is equal to $-\text{var}(X_i)$. What happens as we increase the dimension of the composition?

```{r Simulate high dimension problem}
# Generate random probability vector of 1000 components
probs <- runif(1000)
probs <- probs/sum(probs)

# Generate relative abundances of the 10 components
Xl <- rmultinom(n = 100, 
               size = 1000000, 
               prob = probs)

# Calculate the variance-covariance matrix
covXl <- cov(t(Xl))

# Variances
diag(covXl)[1:10]

# Show the negative bias
diag(covXl) <- 0
rowSums(covXl)[1:10]

```

As shown above mathematically, the fundamental property of the data doesn't change with increasing dimensions.  



----------------------------------------------

$^1$ O.K. *often* is entirely relative here! 


$^2$ To prove this, simply plug in the values of $x_1$ and $x_2 + x_3$ into the formula for covariance:

$$\text{cov}(x_1,\text{ }x_2 + x_3) = \text{E}(x_1 + \text{E}x_1)(x_2 + x_3 + \text{E}(x_2 + x_3)) = \text{E}(x_1 + \text{E}x_1)(x_2 + x_3 + \text{E}x_2 + \text{E}x_3).$$

After multiplying the terms and distributing the expectation we get, 

$$ = \text{E}(x_1x_2) + \text{E}(x_1x_3) + \mu_1\mu_2 + \mu_1\mu_3 $$

Which is equivalent to,

$$ = \text{cov}(x_1, \text{ }x_2) + \text{cov}(x_1, \text{ }x_3)$$

