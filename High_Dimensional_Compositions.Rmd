---
title: "When is a composition not a composition?"
slug: "INSERT_SLUG_HERE"
author: "Dominic LaRoche"
date: "YYYY-MM-DD"
output: markdowntemplates::hugo
---

```{r setup, include=FALSE}
library(knitr)


```



# When can I ignore the compositional property of my data?

My work involves the analysis of data generated by next-generation sequencing (NGS) platforms.  Data from NGS platforms generally falls under the umbrella of 'high dimensional compositions'.  I have often$^1$ been asked how severe the constant sum constraint is for high dimensional data with such a large constraint, when can we safely ignore the compositional nature of the data?  This post is an attempt to answer that question.

Intuitively, if we increase the constraint to infinity we no longer have a constraint.  Is there some constraint $< \infty$ for which a composition ceses to be a composition for all practical purposes?

Also, if you slice up a composition into nearly infinite parts, how much depedence would one expect between the parts.  Intuitively, this dependence would decrease with increasing parts.

So let's put some math behind the intuition and try to answer this question.


## Compositions of very high dimension

Let's start with the dimension question.  How many components are necessary to safely ignore the interdependence between the components?  the problem arises from what John Aitchison described as the "negative bias problem" (note: this is not bias in the traditional estimation sense).  We start with the recognition that every D-part composition can be rescaled by a constant such that the components sum to 1 (*hint - this is forshadowing the discussion of the large sum constraint*).

$$x_1 + x_2 + \ldots + x_D = 1$$

From the definition of covariance ($\text{cov}(X, Y) = \text{E}((X - \mu_x)(Y - \mu_y))$) and the sum constraint we have, 

$$\text{cov}(x_1,\text{ }x_1 + \ldots + x_D) = 0,$$

i.e. the covariance of any individual component with the sum of all the components is 0 (since the covariance of any random variable with a constant is 0).  Moreover, 

$$\text{cov}(x_1,\text{ }x_2 + \ldots + x_D) = \text{cov}(x_1,\text{ }1 - x_1) = -\text{var}(x),$$

which also implies, 

$$\text{cov}(x_1,\text{ }x_2) + \text{cov}(x_1,\text{ }x_3) + \ldots + \text{cov}(x_1,\text{ }x_D) = -\text{var}(x)^{**2}$$

Essenially, every row of the covariance *must* have negative values! This means that at least some covariances are constrained to be negative -- __which means we can't interpret the covariances in the traditional sense__.  As you can see from the above equalities, the 'negative bias' of the covariance matrix does not depend on the dimension, $D$.  


OK, we can't escape the problem of inter-dependence between components entirely, but maybe the problem becomes less severe as $D$ increases?   


----------------------------------------------

$^1$ O.K. *often* is entirely relative here! 


$^2$ To prove this, simply plug in the values of $x_1$ and $x_2 + x_3$ into the formula for covariance:

$$\text{cov}(x_1,\text{ }x_2 + x_3) = \text{E}(x_1 + \text{E}x_1)(x_2 + x_3 + \text{E}(x_2 + x_3)) = \text{E}(x_1 + \text{E}x_1)(x_2 + x_3 + \text{E}x_2 + \text{E}x_3).$$

After multiplying the terms and distributing the expectation we get, 

$$ = \text{E}(x_1x_2) + \text{E}(x_1x_3) + \mu_1\mu_2 + \mu_1\mu_3 $$

Which is equivalent to,

$$ = \text{cov}(x_1, \text{ }x_2) + \text{cov}(x_1, \text{ }x_3)$$

